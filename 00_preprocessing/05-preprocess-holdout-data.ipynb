{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The script will redo all the pre processing steps done in 01-preprocessing folder and create also all the features needed for the prediction\n",
    "\n",
    "####  We regroup the different bookingID in all csv files into one big parquet file\n",
    "#### We suppose that the hold out data will be the same format as the training available online\n",
    "#### For missing information we interpolate the data as long as the duration of the missing values is less than 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tsfresh import extract_features\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "pd.options.display.max_columns = 300\n",
    "\n",
    "#We won't calculate all the possible features with tsfresh, just a part of it\n",
    "features_to_calculate = json.load(open('./feature_calculator.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Configure paths for hold out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mypath = '../data/0-raw_data/holdout/features/'\n",
    "raw_onlyfiles = [f for f in listdir(raw_mypath) if f.startswith('part-')]\n",
    "\n",
    "label_mypath = '../data/0-raw_data/holdout/labels/'\n",
    "# Normally there should be only one file for the holdout label data\n",
    "label_files = [f for f in listdir(label_mypath) if f.startswith('part-')]\n",
    "filename_label = label_files[0]\n",
    "\n",
    "pre_mypath = '../data/1-preprocessed/'\n",
    "pre_filename = 'holdout_features.parquet'\n",
    "\n",
    "ft_mypath = '../data/2-features/'\n",
    "\n",
    "ml_mypath = '../data/3-ml_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Regroup all csv files of the holdout data into one big parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished to regroup all bookingID into 1 dataframe\n",
      "(3227110, 11)\n",
      "Expanding\n",
      "(16579136, 13)\n",
      "Saving preprocessed features...\n",
      "Finished to save file\n"
     ]
    }
   ],
   "source": [
    "print(\"Regrouping all csv in one dataframe...\")\n",
    "\n",
    "cols = ['bookingID','Accuracy','Bearing',\n",
    "        'acceleration_x','acceleration_y','acceleration_z',\n",
    "        'gyro_x','gyro_y','gyro_z','second','Speed']\n",
    "\n",
    "data_cols = ['Accuracy','Bearing',\n",
    "             'acceleration_x','acceleration_y','acceleration_z',\n",
    "             'gyro_x','gyro_y','gyro_z','Speed','second']\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for filename in raw_onlyfiles:\n",
    "    temp = pd.read_csv(raw_mypath+filename)\n",
    "    temp = temp[cols]\n",
    "    temp = temp.loc[~temp.bookingID.isnull()]\n",
    "    df = df.append(temp)\n",
    "    df = df.sort_values(by=['bookingID','second']).reset_index(drop=True)\n",
    "print(\"Finished to regroup all bookingID into 1 dataframe\")\n",
    "print(df.shape)\n",
    "\n",
    "#interpolate data for diff<120\n",
    "df['t0'] = df['second']\n",
    "df['t-1'] = df.groupby(['bookingID'])['second'].shift(+1)\n",
    "df['diff'] = df['t0']-df['t-1']\n",
    "#if diff > 2 minutes we consider that the trip has to be split into 2 sub parts\n",
    "# we create a second bookingID because it doesn't make sense to interpolate for such a long duration\n",
    "df['bookingID2'] = 0\n",
    "df.loc[df['diff']>120,'bookingID2'] = 1\n",
    "df['bookingID2'] = df.groupby(['bookingID'])['bookingID2'].transform('cumsum')\n",
    "\n",
    "df['time'] = pd.to_datetime(df[\"second\"], unit='s')\n",
    "df = df.drop(['t0','t-1','diff'], axis=1)\n",
    "\n",
    "df = df.set_index(['time'])\n",
    "#expand and interpolate trip by booking ID and bookingID2\n",
    "print(\"Expanding\")\n",
    "df = df.groupby(['bookingID','bookingID2'])[data_cols].resample('1S').asfreq().interpolate(method='linear')\n",
    "df = df.reset_index()\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop(['time'], axis=1)\n",
    "\n",
    "#for testing only\n",
    "#df = df.loc[df.bookingID.isin(np.random.choice(df.bookingID.unique(), 100))].reset_index(drop=True)\n",
    "\n",
    "print(\"Saving preprocessed features...\")\n",
    "df.to_parquet(pre_mypath+pre_filename)\n",
    "print(\"Finished to save file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Do the 1st feature engineering for classic ML\n",
    "#### We create features for Xgboost/lightgbm model\n",
    "#### The script takes data from the 1-preprocessed folder and returns a classic dataframe in './data/2-features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_ml1(_filename, _filename_label):\n",
    "    \n",
    "    data_cols = ['Accuracy','Bearing',\n",
    "                 'acceleration_x','acceleration_y','acceleration_z',\n",
    "                 'gyro_x','gyro_y','gyro_z',\n",
    "                 'Speed']\n",
    "    \n",
    "    print(\"Processing %s\" %_filename)\n",
    "    df = pd.read_parquet(pre_mypath+_filename)\n",
    "    dt = pd.read_csv(label_mypath+_filename_label)\n",
    "    dt = dt.groupby('bookingID', as_index=False)['label'].max()\n",
    "    \n",
    "    #add labels\n",
    "    df = df.merge(dt, on='bookingID', how='left')\n",
    "    #we reset second to start from 0 for every (bookingID, bookingID2)\n",
    "    df = df.sort_values(by=['bookingID','second']).reset_index(drop=True)\n",
    "    df['second'] = df.groupby(['bookingID'])['second'].cumcount()\n",
    "    \n",
    "    #manual feature creation\n",
    "    #every acceleration >= 9.8m/s2 is considered as harsh\n",
    "    for col in ['acceleration_x','acceleration_y','acceleration_z']:\n",
    "        df['harsh_'+col] = (np.abs(df[col])>= 9.8).astype(int)\n",
    "        \n",
    "    # calculate the slope for gyro and speed\n",
    "    for col in ['gyro_x','gyro_y','gyro_z','Speed']:\n",
    "        for i in range(1,6):\n",
    "            df['d_'+col+\"-\"+str(i)] = df.groupby(['bookingID','bookingID2'])[col].shift(+i)\n",
    "            df['d_'+col+\"-\"+str(i)] = (df['d_'+col+\"-\"+str(i)] - df[col])/i\n",
    "    \n",
    "    # for the slopes, flag when they are above the 80 percentile\n",
    "    for d_col in [col for col in df.columns if col.startswith('d_') ]:\n",
    "        df['harsh_positive_'+d_col] = ((df[d_col]>=df.loc[df[d_col]>0,col].quantile(.8)) & (df[col]>0)).astype(int)\n",
    "        df['harsh_negative_'+d_col] = ((df[col]<=df.loc[df[col]<0,col].quantile(.2)) & (df[col]<0)).astype(int)\n",
    "        \n",
    "    df2 = df.copy()\n",
    "    df2 = df2.drop(['bookingID2','second','label'], axis=1)\n",
    "    df2 = df2.groupby(['bookingID']).agg(['count', 'mean', 'std', 'sum', 'min', 'median', 'max'])\n",
    "    df2.columns = ['_'.join(col).strip() for col in df2.columns.values]\n",
    "    \n",
    "    for col in data_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    \n",
    "    df2 = df2.reset_index()\n",
    "\n",
    "    #add labels\n",
    "    df2 = df2.merge(dt, on='bookingID', how='left')\n",
    "    #save file to parquet\n",
    "    df2.to_parquet(ft_mypath+_filename.replace('.parquet','')+'_xgboost1.parquet')\n",
    "    print(\"Finished with %s \" %_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1st batch of features for Xgboost/Lgbm...\n",
      "Processing holdout_features.parquet\n",
      "Finished with holdout_features.parquet \n",
      "Finished to create features for XGBoost/lgbm.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating 1st batch of features for Xgboost/Lgbm...\")\n",
    "create_matrix_ml1(pre_filename, filename_label)\n",
    "print(\"Finished to create features for XGBoost/lgbm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Do the 2nd feature engineering for classic ML\n",
    "### Becareful this part make take a while (4 hours on an EC2 r4.4xlarge instance)\n",
    "#### We create additionnal features for Xgboost/lightgbm model using tsfresh\n",
    "#### The script takes data from the 1-preprocessed folder and returns an additionnal classic dataframe in './data/2-features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_ml2(_filename, _filename_label):\n",
    "    \n",
    "    data_cols = ['Accuracy','Bearing',\n",
    "                 'acceleration_x','acceleration_y','acceleration_z','acceleration',\n",
    "                 'gyro_x','gyro_y','gyro_z','gyro',\n",
    "                 'Speed']\n",
    "    \n",
    "    print(\"Processing %s\" %_filename)\n",
    "    df = pd.read_parquet(pre_mypath+_filename)\n",
    "    dt = pd.read_csv(label_mypath+_filename_label)\n",
    "    dt = dt.groupby('bookingID', as_index=False)['label'].max()\n",
    "\n",
    "    df['acceleration'] = (df[[\"acceleration_x\", \"acceleration_y\", \"acceleration_z\"]]**2).sum(axis=1)**0.5\n",
    "    df['gyro'] = (df[[\"gyro_x\", \"gyro_y\", \"gyro_z\"]]**2).sum(axis=1)**0.5\n",
    "\n",
    "    #we reset second to start from 0 for every (bookingID)\n",
    "    df = df.sort_values(by=['bookingID','second']).reset_index(drop=True)\n",
    "    df['second'] = df.groupby(['bookingID'])['second'].cumcount()\n",
    "    #df = df.loc[df.bookingID.isin(df.bookingID.unique()[:300])].copy()\n",
    "    df = df.drop(['Accuracy'], axis=1)\n",
    "    \n",
    "    extracted_features = extract_features(df, column_id=\"bookingID\", column_sort=\"second\", \n",
    "                                          default_fc_parameters=features_to_calculate, n_jobs=14)\n",
    "\n",
    "    extracted_features = extracted_features.reset_index().rename(columns={'id':'bookingID'})\n",
    "    #add labels\n",
    "    df = extracted_features.merge(dt, on='bookingID', how='left')\n",
    "    #save file to parquet\n",
    "    df.to_parquet(ft_mypath+_filename.replace('.parquet','')+'_xgboost2.parquet')\n",
    "    print(\"Finished with %s \" %_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 2nd batch of features for Xgboost/Lgbm...\n",
      "Processing holdout_features.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 69/69 [01:40<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with holdout_features.parquet \n",
      "Finished to create additional features for XGBoost/lgbm.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating 2nd batch of features for Xgboost/Lgbm...\")\n",
    "create_matrix_ml2(pre_filename, filename_label)\n",
    "print(\"Finished to create additional features for XGBoost/lgbm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Regroup holdout_features_xgboost1.parquet and holdout_features_xgboost2.parquet into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrouping 1st and 2nd batches of features for Xgboost/Lgbm...\n",
      "Finished to create final ml dataset for XGBoost/lgbm.\n"
     ]
    }
   ],
   "source": [
    "print(\"Regrouping 1st and 2nd batches of features for Xgboost/Lgbm...\")\n",
    "df_xgb1 = pd.read_parquet(ft_mypath+'holdout_features_xgboost1.parquet')\n",
    "df_xgb1 = df_xgb1.drop('label', axis=1)\n",
    "df_xgb2 = pd.read_parquet(ft_mypath+'holdout_features_xgboost2.parquet')\n",
    "keep_cols2 = ['bookingID']+[col for col in df_xgb2.columns if col not in df_xgb1.columns]\n",
    "df_xgb = df_xgb1.merge(df_xgb2[keep_cols2], on='bookingID', how='left')\n",
    "\n",
    "df_xgb.to_parquet(ml_mypath+'holdout_xgb.parquet')\n",
    "\n",
    "print(\"Finished to create final ml dataset for XGBoost/lgbm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
